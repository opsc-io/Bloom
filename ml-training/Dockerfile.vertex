# Vertex AI Custom Prediction Container
# Downloads model from GCS at startup and serves via FastAPI
# Following Vertex AI custom container requirements

FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

WORKDIR /app

# Install system dependencies including gcloud
RUN apt-get update && apt-get install -y \
    git \
    curl \
    gnupg \
    && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \
    && echo "deb https://packages.cloud.google.com/apt cloud-sdk main" | tee /etc/apt/sources.list.d/google-cloud-sdk.list \
    && apt-get update && apt-get install -y google-cloud-sdk \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy inference code
COPY model.py .
COPY inference.py .
COPY startup.sh .
RUN chmod +x startup.sh

# Create model directory
RUN mkdir -p /model

# Expose Vertex AI required port
EXPOSE 8080

# Environment variables
ENV PYTHONUNBUFFERED=1
ENV MODEL_DIR=/model
ENV MODEL_NAME=xlm-roberta-large
ENV HF_HOME=/tmp/huggingface
ENV TRANSFORMERS_CACHE=/tmp/huggingface
ENV TOKENIZERS_PARALLELISM=false
ENV AIP_HTTP_PORT=8080

# Vertex AI health check endpoint (required)
ENV AIP_HEALTH_ROUTE=/health
ENV AIP_PREDICT_ROUTE=/predict

# Run startup script which downloads model and starts server
CMD ["./startup.sh"]
